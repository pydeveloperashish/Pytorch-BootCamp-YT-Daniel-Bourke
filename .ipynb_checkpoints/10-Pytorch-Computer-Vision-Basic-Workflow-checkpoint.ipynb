{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6a98c9",
   "metadata": {},
   "source": [
    "`torchvision` - Base Domain Library for Pytorch Computer Vision.\n",
    "\n",
    "`torchvision.datasets` - Get Datasets and data loading functions for computer vision here. \n",
    "\n",
    "`torchvision.models` - Get pre-trained computer vision models that you can leverage for your own problems.\n",
    "\n",
    "`torchvision.transforms` - Functions for manipulating your vision data (images) to be suitable for use with \n",
    "                               an ML model.\n",
    "                               \n",
    "`torch.utils.data.Dataset` - Base dataset class for Pytorch.\n",
    "\n",
    "`torch.utils.data.DataLoader` - Creates a Python iterable over a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8ec32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd2dd5",
   "metadata": {},
   "source": [
    "# 1. Getting a Dataset\n",
    "\n",
    "The dataset we are using is Fashion MNIST Dataset from torchvision.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5834d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(root = \"Datasets\", train = True, download = True, \n",
    "                                   transform = transforms.ToTensor(),   # how do want to transform the data\n",
    "                                   target_transform = None) # How do we want to transforms the label/targets?\n",
    "\n",
    "\n",
    "test_data = datasets.FashionMNIST(root = \"Datasets\", train = False, download = True, \n",
    "                                   transform = transforms.ToTensor(),   # how do want to transform the data\n",
    "                                   target_transform = None) # How do we want to transforms the label/targets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017d8ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb5b54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
       "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
       "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
       "           0.0157, 0.0000, 0.0000, 0.0118],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
       "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0471, 0.0392, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
       "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
       "           0.3020, 0.5098, 0.2824, 0.0588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
       "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
       "           0.5529, 0.3451, 0.6745, 0.2588],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
       "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
       "           0.4824, 0.7686, 0.8980, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
       "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
       "           0.8745, 0.9608, 0.6784, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
       "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
       "           0.8627, 0.9529, 0.7922, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
       "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
       "           0.8863, 0.7725, 0.8196, 0.2039],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
       "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
       "           0.9608, 0.4667, 0.6549, 0.2196],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
       "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
       "           0.8510, 0.8196, 0.3608, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
       "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
       "           0.8549, 1.0000, 0.3020, 0.0000],\n",
       "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
       "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
       "           0.8784, 0.9569, 0.6235, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
       "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
       "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
       "           0.9137, 0.9333, 0.8431, 0.0000],\n",
       "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
       "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
       "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
       "           0.8627, 0.9098, 0.9647, 0.0000],\n",
       "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
       "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
       "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
       "           0.8706, 0.8941, 0.8824, 0.0000],\n",
       "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
       "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
       "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
       "           0.8745, 0.8784, 0.8980, 0.1137],\n",
       "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
       "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
       "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
       "           0.8627, 0.8667, 0.9020, 0.2627],\n",
       "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
       "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
       "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
       "           0.7098, 0.8039, 0.8078, 0.4510],\n",
       "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
       "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
       "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
       "           0.6549, 0.6941, 0.8235, 0.3608],\n",
       "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
       "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
       "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
       "           0.7529, 0.8471, 0.6667, 0.0000],\n",
       "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
       "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
       "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
       "           0.3882, 0.2275, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
       "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2945a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f275ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82ee369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e43b35",
   "metadata": {},
   "source": [
    "# 2. Check the Shape of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63073215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96091e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09ec89a4",
   "metadata": {},
   "source": [
    "# 3. Visualize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0c999c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea3ae77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4a2c4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "plt.imshow(image.squeeze())   # squeeze to get rid of that extra dimension that is channel dimension\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff207c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap = \"gray\")  \n",
    "plt.title(train_data.classes[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fef683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02f5116b",
   "metadata": {},
   "source": [
    "# 4. Prepare DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93a77c",
   "metadata": {},
   "source": [
    "Right now, our data is in the form of Pytorch Datasets.\n",
    "\n",
    "DataLoader turns our dataset into a Python iterable.\n",
    "\n",
    "More Specifically, we want to turn our data into batches (or mini-batches).\n",
    "\n",
    "Why would we do this?\n",
    "\n",
    "     1) It is more computationally efficient, as in, your computing hardware may not be able to look (store in               memory) at 60000 images in one hit. So we break it down to 32 images at a time (batch size of 32).\n",
    "     \n",
    "     2) It gives our neural network more chances to update its gradients per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd62927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset FashionMNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: Datasets\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset FashionMNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: Datasets\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84801c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b66866f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7fc488520460>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7fc488520c70>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Turn datasets into iterables (batches)\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_data,\n",
    "                              batch_size = BATCH_SIZE,\n",
    "                              shuffle = True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset = test_data,\n",
    "                             batch_size = BATCH_SIZE,\n",
    "                             shuffle = False)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae322fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1875, 313)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4beca878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1875.0, 312.5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60000/32, 10000/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f50e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d450fcaf",
   "metadata": {},
   "source": [
    "# 5. Building a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f4cd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_shape: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = input_shape, out_features = hidden_shape),\n",
    "            nn.Linear(in_features = hidden_shape, out_features = output_shape)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5edae973",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = FashionMNISTModelV0(input_shape = 784,  # this is 28*28\n",
    "                              hidden_shape = 16,\n",
    "                              output_shape = len(train_data.class_to_idx)).to(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fc3678d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=16, bias=True)\n",
       "    (2): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd495d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0204, -0.2367,  0.2342,  0.0310,  0.0227, -0.0970,  0.2339,  0.0446,\n",
      "          0.0219, -0.2840]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# testing our mode\n",
    "dummy_x = torch.rand([1, 1, 28, 28])\n",
    "print(model_0(dummy_x))\n",
    "print()\n",
    "print(model_0(dummy_x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b2db9d",
   "metadata": {},
   "source": [
    "Model is working, lets go Ahead !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53924e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bdd2799",
   "metadata": {},
   "source": [
    "# 6. Pickup Loss Function, Optimizer, Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e39e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model_0.parameters(),\n",
    "                        lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbfa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4421164",
   "metadata": {},
   "source": [
    "# 7. Creating a Training and Testing Loop on bacthes of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da24a6d",
   "metadata": {},
   "source": [
    "1. Loop through Epochs.\n",
    "2. Loop through training batches, perform training steps, calculate the train loss per batch.\n",
    "3. Loop through testing batches, perform testing steps, calculate the test loss per batch.\n",
    "4. Print out what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c625a38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecd582b8c334c2e8a8a730a63511500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looked at 0 / 60000 samples.\n",
      "Looked at 12800 / 60000 samples.\n",
      "Looked at 25600 / 60000 samples.\n",
      "Looked at 38400 / 60000 samples.\n",
      "Looked at 51200 / 60000 samples.\n",
      "\n",
      "Train Loss: 0.4381 | Test Loss: 0.4698, Test Acc: 83.40\n",
      "\n",
      "Looked at 0 / 60000 samples.\n",
      "Looked at 12800 / 60000 samples.\n",
      "Looked at 25600 / 60000 samples.\n",
      "Looked at 38400 / 60000 samples.\n",
      "Looked at 51200 / 60000 samples.\n",
      "\n",
      "Train Loss: 0.4318 | Test Loss: 0.4628, Test Acc: 83.65\n",
      "\n",
      "Looked at 0 / 60000 samples.\n",
      "Looked at 12800 / 60000 samples.\n",
      "Looked at 25600 / 60000 samples.\n",
      "Looked at 38400 / 60000 samples.\n",
      "Looked at 51200 / 60000 samples.\n",
      "\n",
      "Train Loss: 0.4264 | Test Loss: 0.4553, Test Acc: 83.93\n",
      "\n",
      "Looked at 0 / 60000 samples.\n",
      "Looked at 12800 / 60000 samples.\n",
      "Looked at 25600 / 60000 samples.\n",
      "Looked at 38400 / 60000 samples.\n",
      "Looked at 51200 / 60000 samples.\n",
      "\n",
      "Train Loss: 0.4220 | Test Loss: 0.4515, Test Acc: 84.05\n",
      "\n",
      "Looked at 0 / 60000 samples.\n",
      "Looked at 12800 / 60000 samples.\n",
      "Looked at 25600 / 60000 samples.\n",
      "Looked at 38400 / 60000 samples.\n",
      "Looked at 51200 / 60000 samples.\n",
      "\n",
      "Train Loss: 0.4182 | Test Loss: 0.4510, Test Acc: 84.14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set the no of epochs\n",
    "EPOCHS = 5\n",
    "    \n",
    "# Create training and testing loop\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    \n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, Y) in enumerate(train_dataloader):\n",
    "        model_0.train()   # training mode ON\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        Y_pred = model_0(X)\n",
    "        \n",
    "        # 2. Calculate loss (per batch)\n",
    "        \n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "        train_loss += loss \n",
    "        \n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print out what's happening\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)} / {len(train_dataloader.dataset)} samples.\")\n",
    "    \n",
    "    # Divide total train_loss by length of train_dataloader\n",
    "    train_loss /= len(train_dataloader)   # finding ot avg train loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Testing\n",
    "    model_0.eval() # Testing Mode ON\n",
    "    \n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X_test, Y_test in test_dataloader:\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X_test)\n",
    "            \n",
    "            # 2. Calculate loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, Y_test)\n",
    "            \n",
    "            # 3. Calculate Accuracy\n",
    "            test_acc += accuracy_fn(y_true = Y_test, y_pred = test_pred.argmax(dim = 1))\n",
    "            \n",
    "        # Calculate the test loss avg. per batch\n",
    "        test_loss /= len(test_dataloader)\n",
    "        \n",
    "        # Calculate the test acc avg. per batch\n",
    "        test_acc /= len(test_dataloader)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Print out what's happening\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b95d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "158dacd2",
   "metadata": {},
   "source": [
    "# 8. Building a Better Model with Non-Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb964163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# writing a device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817a152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88d9b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model with non-linear and linear layers\n",
    "\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape: int,\n",
    "                 hidden_units: int,\n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features = input_shape, out_features = hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = hidden_units, out_features = output_shape)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5eb8ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV1(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=16, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = FashionMNISTModelV1(input_shape = 784, \n",
    "                              hidden_units = 16, \n",
    "                              output_shape = len(train_data.class_to_idx)\n",
    "                             ).to(device)\n",
    "\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6eb81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c06b2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickup Loss Function, Optimizer, Evaluation Metrics\n",
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params = model_0.parameters(),\n",
    "                        lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ca494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddf2594b",
   "metadata": {},
   "source": [
    "Creating a Training and Testing Loop on bacthes of data.\n",
    "\n",
    "Lets functionize this:-\n",
    "\n",
    "1) Training Loop - train_step()\n",
    "\n",
    "2) Testing Loop - test_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "312e5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    \n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    model.train()   # training mode ON\n",
    "    \n",
    "    \n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, Y) in enumerate(data_loader):\n",
    "        \n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        Y_pred = model(X)\n",
    "        \n",
    "        # 2. Calculate loss (per batch)\n",
    "        \n",
    "        loss = loss_fn(Y_pred, Y)\n",
    "        train_loss += loss \n",
    "        train_acc += accuracy_fn(y_true = Y, y_pred = Y_pred.argmax(dim = 1))\n",
    "        \n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    # Divide total train_loss and acc by length of train_dataloader\n",
    "    train_loss /= len(data_loader)   # finding ot avg train loss\n",
    "    train_acc /= len(data_loader)    # finding ot avg train acc\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.5f} | Train Acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a5bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29bc64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              data_loader: torch.utils.data.DataLoader,\n",
    "              loss: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    \n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    model.eval()  # put the model in Eval\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X,Y in data_loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            \n",
    "            # 1. Forward Pass\n",
    "            test_pred = model(X)\n",
    "            \n",
    "            # 2. Calculate the loss/acc\n",
    "            test_loss += loss_fn(test_pred, Y)\n",
    "            test_acc += accuracy_fn(y_true = Y, y_pred = test_pred.argmax(dim = 1))\n",
    "            \n",
    "            \n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        \n",
    "        print(f\"Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93080d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eecf1d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19ecb660e6a47308d590c0033a207a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.31074 | Train Acc: 6.49%\n",
      "Test Loss: 2.31171 | Test Acc: 6.35%\n",
      "\n",
      "Train Loss: 2.31075 | Train Acc: 6.49%\n",
      "Test Loss: 2.31171 | Test Acc: 6.35%\n",
      "\n",
      "Train Loss: 2.31074 | Train Acc: 6.49%\n",
      "Test Loss: 2.31171 | Test Acc: 6.35%\n",
      "\n",
      "Train Loss: 2.31074 | Train Acc: 6.49%\n",
      "Test Loss: 2.31171 | Test Acc: 6.35%\n",
      "\n",
      "Train Loss: 2.31074 | Train Acc: 6.49%\n",
      "Test Loss: 2.31171 | Test Acc: 6.35%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the training and testing\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(model = model_1,\n",
    "               data_loader = train_dataloader,\n",
    "               loss = loss_fn,\n",
    "               optimizer = optimizer,\n",
    "               accuracy_fn = accuracy_fn,\n",
    "               device = device)\n",
    "    \n",
    "    test_step(model = model_1,\n",
    "              data_loader = test_dataloader,\n",
    "              loss = loss_fn,\n",
    "              accuracy_fn = accuracy_fn,\n",
    "              device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d85916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "959f5395",
   "metadata": {},
   "source": [
    "# Non linear model isnt giving Good Results in this particular Test Case !!!\n",
    "# Lets try to use CNN in the Next Part !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289817d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433c7bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b224dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b28457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26df25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343d12a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
